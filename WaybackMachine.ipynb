{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wayback Machine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "import glob\n",
    "import time\n",
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOURCES = []\n",
    "DOWNLOAD_ARCHIVES = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_wayback_versions(\n",
    "    url,\n",
    "    from_timestamp=\"\",\n",
    "    to_timestamp=\"\",\n",
    "    max_versions=10,\n",
    "    include_content=False,\n",
    "    initial_throttle_seconds=1,  # Initial throttle time in seconds\n",
    "    max_retries=5,  # Maximum number of retries for HTTP requests\n",
    "):\n",
    "    \"\"\"\n",
    "    Fetch past versions of a URL from the Wayback Machine and optionally the content,\n",
    "    with exponential backoff for handling request retries.\n",
    "\n",
    "    Parameters:\n",
    "    - url (str): The URL to fetch past versions for.\n",
    "    - from_timestamp (str): Start timestamp in format YYYYmmddHHMMSS. Empty means no start limit.\n",
    "    - to_timestamp (str): End timestamp in format YYYYmmddHHMMSS. Empty means no end limit.\n",
    "    - max_versions (int): Maximum number of versions to fetch.\n",
    "    - include_content (bool): Whether to fetch the content of the archived URL.\n",
    "    - initial_throttle_seconds (float): Initial number of seconds to wait before retrying a request. Default is 1.\n",
    "    - max_retries (int): Maximum number of retries for a request. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "    - list of dicts: Each dict contains 'timestamp', 'archive_url', and optionally 'archive_content'.\n",
    "    \"\"\"\n",
    "    print(\n",
    "        f\"Fetching versions for URL: {url} from {from_timestamp} to {to_timestamp} with max versions {max_versions}\"\n",
    "    )\n",
    "\n",
    "    base_url = \"http://web.archive.org/cdx/search/cdx\"\n",
    "    params = {\n",
    "        \"url\": url,\n",
    "        \"output\": \"json\",\n",
    "        \"from\": from_timestamp,\n",
    "        \"to\": to_timestamp,\n",
    "        \"limit\": max_versions,\n",
    "    }\n",
    "    versions = []\n",
    "\n",
    "    def make_request_with_exponential_backoff(\n",
    "        url,\n",
    "        params=None,\n",
    "        max_retries=max_retries,\n",
    "        initial_delay=initial_throttle_seconds,\n",
    "    ):\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                print(\n",
    "                    f\"Attempting request to {url} (Attempt {attempt + 1}/{max_retries})\"\n",
    "                )\n",
    "                response = requests.get(url, params=params)\n",
    "                response.raise_for_status()  # Raise an HTTPError if the response was an error\n",
    "                print(\"Request successful\")\n",
    "                return response  # Success\n",
    "            except requests.RequestException as e:\n",
    "                print(f\"Request failed: {e}\")\n",
    "                if attempt < max_retries - 1:\n",
    "                    sleep_time = initial_delay * (2**attempt)  # Exponential backoff\n",
    "                    print(f\"Retrying in {sleep_time} seconds...\")\n",
    "                    time.sleep(sleep_time)\n",
    "                else:\n",
    "                    print(\"Max retries reached, giving up.\")\n",
    "                    raise  # Reraise the last exception\n",
    "\n",
    "    try:\n",
    "        response = make_request_with_exponential_backoff(base_url, params=params)\n",
    "        data = response.json()\n",
    "\n",
    "        if not data or len(data) < 2:\n",
    "            print(\"No data found for the given URL and timestamps.\")\n",
    "            return []\n",
    "\n",
    "        for item in data[1:]:\n",
    "            version_info = {\n",
    "                \"timestamp\": item[1],\n",
    "                \"url\": url,\n",
    "                \"archive_url\": f\"https://web.archive.org/web/{item[1]}/{url}\",\n",
    "            }\n",
    "            print(f\"Found version: {version_info['timestamp']}\")\n",
    "            if include_content:\n",
    "                try:\n",
    "                    print(f\"Fetching content for {version_info['archive_url']}\")\n",
    "                    content_response = make_request_with_exponential_backoff(\n",
    "                        version_info[\"archive_url\"]\n",
    "                    )\n",
    "                    version_info[\"archive_content\"] = content_response.text\n",
    "                    print(\"Content fetched successfully\")\n",
    "                except requests.RequestException as e:\n",
    "                    print(\n",
    "                        f\"Error fetching content for {version_info['archive_url']}: {e}\"\n",
    "                    )\n",
    "                    version_info[\"archive_content\"] = \"Error fetching content\"\n",
    "            versions.append(version_info)\n",
    "\n",
    "        return versions\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching data from Wayback Machine: {e}\")\n",
    "\n",
    "        return versions\n",
    "\n",
    "\n",
    "def load_feeds_as_dataframe(glob_pattern):\n",
    "    \"\"\"\n",
    "    Load all RSS feeds matching a glob pattern into a single Pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - glob_pattern (str): The glob pattern to match files.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A Pandas DataFrame containing the concatenated data from all matched XML files.\n",
    "    \"\"\"\n",
    "\n",
    "    files = glob.glob(glob_pattern)\n",
    "\n",
    "    dfs = []\n",
    "\n",
    "    for file in files:\n",
    "        df = pd.read_xml(file, xpath=\".//item\")\n",
    "\n",
    "        dfs.append(df)\n",
    "\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "def save_content(file_path, content):\n",
    "    \"\"\"\n",
    "    Saves content to a file, creating parent directories if they don't exist.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): The path to the file where content will be saved.\n",
    "    - content (str): The content to save to the file.\n",
    "    \"\"\"\n",
    "    # Convert the file_path string to a Path object\n",
    "    path = Path(file_path)\n",
    "\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with path.open(mode=\"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(content)\n",
    "\n",
    "\n",
    "def url_to_filepath(url, base_dir=\"web_content\", timestamp=None):\n",
    "    \"\"\"\n",
    "    Converts a URL to a file path, using a base directory, and optionally includes a timestamp\n",
    "    for versioning.\n",
    "\n",
    "    Parameters:\n",
    "    - url (str): The URL to convert.\n",
    "    - base_dir (str): The base directory where the content will be stored.\n",
    "    - timestamp (str, optional): A timestamp string to include in the file path for versioning.\n",
    "\n",
    "    Returns:\n",
    "    - Path: A pathlib.Path object representing the file path.\n",
    "    \"\"\"\n",
    "    parsed_url = urlparse(url)\n",
    "\n",
    "    domain_name = parsed_url.netloc\n",
    "    path = parsed_url.path\n",
    "\n",
    "    clean_path = path.strip(\"/\")\n",
    "\n",
    "    path_parts = clean_path.split(\"/\") if clean_path else []\n",
    "\n",
    "    # Construct the file path from the base directory, domain name, and path parts\n",
    "    if path_parts:\n",
    "        # Extract the last part as the filename\n",
    "        filename = path_parts.pop()\n",
    "        # Identify the file extension\n",
    "        name, dot, extension = filename.partition(\".\")\n",
    "        # Insert the timestamp before the extension (if any)\n",
    "        if timestamp:\n",
    "            filename = (\n",
    "                f\"{name}{dot}{timestamp}{dot}{extension}\"\n",
    "                if dot\n",
    "                else f\"{name}{dot}{timestamp}\"\n",
    "            )\n",
    "        else:\n",
    "            filename = f\"{name}{dot}{extension}\"\n",
    "        path_parts.append(filename)\n",
    "\n",
    "    filepath = Path(base_dir, domain_name, *path_parts)\n",
    "\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Archives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DOWNLOAD_ARCHIVES:\n",
    "    versions = []\n",
    "    for source in SOURCES:\n",
    "        source_versions = fetch_wayback_versions(\n",
    "            source,\n",
    "            max_versions=5,\n",
    "            include_content=True,\n",
    "            initial_throttle_seconds=2,\n",
    "            max_retries=2,\n",
    "        )\n",
    "        versions.extend(source_versions)\n",
    "    for version in versions:\n",
    "        url = version[\"url\"]\n",
    "        timestamp = version[\"timestamp\"]\n",
    "        content = version[\"archive_content\"]\n",
    "        filepath = url_to_filepath(url, base_dir=\"data\", timestamp=timestamp)\n",
    "        save_content(filepath, content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Headlines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc_feeds = load_feeds_as_dataframe(\"./data/abcnews.go.com/abcnews/*\")\n",
    "bbc_feeds = load_feeds_as_dataframe(\"./data/feeds.bbci.co.uk/news/*.xml\")\n",
    "cnn_feeds = load_feeds_as_dataframe(\"./data/rss.cnn.com/rss/*.rss\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WaybackMachine-BiQ6x-X5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
